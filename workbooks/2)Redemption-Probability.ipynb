{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45218b28-f562-4baa-93a2-d045e1f4a641",
   "metadata": {},
   "source": [
    "# Coupon Probability Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48693ff7-f9d5-487c-9caa-77056c526b4a",
   "metadata": {},
   "source": [
    "Here I would like to run a model for every customer in that week based on the maximum and minimum is it possible to provide them. I would like a probability number that then are going to accept the offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d594feb-187a-4c5b-b81b-36aba00602d6",
   "metadata": {},
   "source": [
    "First I need to import tghe sanitised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a9b0dc-e84a-4e3b-83fc-18ce8027ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03167700-2ce1-437e-a0ba-9e7ed77b98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/raw/TestingData.csv\")\n",
    "df_train = pd.read_csv(\"../data/raw/HistoricalTrainingData.csv\")\n",
    "df_gradient = pd.read_csv(\"../data/processed/customer_sensitivity_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90df3fc8-348e-4508-8652-6277916b95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is available.\n",
      "Number of available GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA is available\n",
    "    print(\"CUDA (GPU support) is available.\")\n",
    "    \n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Get the name of the GPU device\n",
    "    gpu_name = torch.cuda.get_device_name(0)  # Assuming you have at least one GPU\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "    # Set the device to GPU (assuming you want to use the first GPU)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # CUDA is not available, use CPU\n",
    "    print(\"CUDA (GPU support) is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281d424-099c-46a2-b56b-17b3c148ce28",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9acd70a-61bd-4cca-9109-67da107625f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\projects\\python\\coupon-optimisation\\workbooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ad5a52-de4e-4fa2-b745-fd9664b4ff65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects/python/coupon-optimisation\\src\\preprocessing\\preprocessing.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  agg_features['MedianSpendPerOfferAmt'].replace([np.inf, -np.inf], 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:/projects/python/coupon-optimisation')\n",
    "from src.preprocessing import preprocess_and_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, features = preprocess_and_split(df_train, df_test, df_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e201e9-9d9f-4a75-81f5-11fe72f57011",
   "metadata": {},
   "source": [
    "# Machine Learning Model Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b6218e-223b-4923-8454-fae374c2fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.XGBoost import XGBoostTrainer\n",
    "from src.models.NeuralNetwork import NeuralNetworkTrainer\n",
    "from src.models.LogisticRegression import LogisticRegressionTrainer\n",
    "from src.models.GradientBoostClassifier import GBMTrainer\n",
    "from src.models.CatBoost import CatBoostTrainer\n",
    "from src.models.LightGradientBoost import LightGBMTrainer\n",
    "from src.models.BayesianTrainer import BayesianModelTrainer\n",
    "from src.models.KNN import KNNTrainer\n",
    "from src.models.ElasticNet import ElasticNetTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cc415-5b81-4253-8157-215d14dfe474",
   "metadata": {},
   "source": [
    "Set the evaluation metrics and define the mlruns directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110d1690-8bcf-4e9c-aafb-dbb614dd65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util.models_util import evaluate_model, log_metrics\n",
    "scaler_path = '../models/scalers/scaler.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db24452-d2d8-4635-a178-af2567113ad4",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acaa4804-68d4-4543-bf46-54107b4a5be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.43273434042930603\n",
      "Epoch 2/25, Loss: 0.2942943871021271\n",
      "Epoch 3/25, Loss: 0.32934999465942383\n",
      "Epoch 4/25, Loss: 0.41253894567489624\n",
      "Epoch 5/25, Loss: 0.3412223756313324\n",
      "Epoch 6/25, Loss: 0.16870488226413727\n",
      "Epoch 7/25, Loss: 0.38206928968429565\n",
      "Epoch 8/25, Loss: 0.3434986472129822\n",
      "Epoch 9/25, Loss: 0.44097232818603516\n",
      "Epoch 10/25, Loss: 0.2828368544578552\n",
      "Epoch 11/25, Loss: 0.4011697769165039\n",
      "Epoch 12/25, Loss: 0.26904433965682983\n",
      "Epoch 13/25, Loss: 0.3970431089401245\n",
      "Epoch 14/25, Loss: 0.33614271879196167\n",
      "Epoch 15/25, Loss: 0.45306816697120667\n",
      "Epoch 16/25, Loss: 0.32941967248916626\n",
      "Epoch 17/25, Loss: 0.26086458563804626\n",
      "Epoch 18/25, Loss: 0.31626707315444946\n",
      "Epoch 19/25, Loss: 0.4740070700645447\n",
      "Epoch 20/25, Loss: 0.6471055150032043\n",
      "Epoch 21/25, Loss: 0.3181081712245941\n",
      "Epoch 22/25, Loss: 0.4514879584312439\n",
      "Epoch 23/25, Loss: 0.485196590423584\n",
      "Epoch 24/25, Loss: 0.453777939081192\n",
      "Epoch 25/25, Loss: 0.3362463116645813\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/scalers/scaler.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Predict probabilities on the test set\u001b[39;00m\n\u001b[0;32m     17\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate the model and log metrics\u001b[39;00m\n\u001b[0;32m     21\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(y_test, probabilities)\n",
      "File \u001b[1;32mC:\\projects/python/coupon-optimisation\\src\\models\\NeuralNetwork.py:60\u001b[0m, in \u001b[0;36mNeuralNetworkTrainer.save_model\u001b[1;34m(self, directory, scaler_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), filename)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and scaler saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bayes-optimisation\\lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/scalers/scaler.joblib'"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "nn_model_path = '../models/'\n",
    "\n",
    "# Ensure the directory for the model exists\n",
    "os.makedirs(os.path.dirname(nn_model_path), exist_ok=True)\n",
    "\n",
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = NeuralNetworkTrainer(num_features=X_train.shape[1])\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='Neural Network PyTorch n-1/5'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "\n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d0f77-bbfe-4920-a7dd-0d1c91e89f3d",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6f5a13-f1f7-4f98-a675-43038522c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "Model saved to ../models/trained-models/LogisticRegressionTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.685544250335957, 'recall': 0.6430758148748424, 'roc_auc': 0.8289644351614488, 'f1': 0.6636312952982718, 'TP': 3571, 'TN': 8656, 'FP': 1638, 'FN': 1982}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = LogisticRegressionTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='Logistic Regression + n1/5'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "\n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0ab6e-c555-44d8-a8b8-6a363738b306",
   "metadata": {},
   "source": [
    "## XGBoost Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85163f2e-ea12-4ae5-98d2-0948537cbdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtren\\anaconda3\\envs\\bayes-optimisation\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "Model saved to ../models/trained-models/XGBoostTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.7050359712230215, 'recall': 0.68827660723933, 'roc_auc': 0.8533935450991349, 'f1': 0.6965554948059047, 'TP': 3822, 'TN': 8695, 'FP': 1599, 'FN': 1731}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = XGBoostTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='XGBoost + n1/5'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "\n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066645e-9eda-4ae8-8ade-5bb1c8fa81ed",
   "metadata": {},
   "source": [
    "## Gradient Boosted Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3262ae5c-ea2e-4516-b823-3c4b2e777214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "Model saved to ../models/trained-models/GBMTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.7272897546356996, 'recall': 0.6992616603637674, 'roc_auc': 0.8664968895211906, 'f1': 0.7130003672420125, 'TP': 3883, 'TN': 8838, 'FP': 1456, 'FN': 1670}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = GBMTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='GBM Optuna F1-Score'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "    \n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ff5344-80df-4d5b-99b1-9eae7719c048",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03240ee-2939-4c47-ace1-c34a0316ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost model trained.\n",
      "Model saved to ../models/trained-models/CatBoostTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.7034344902780302, 'recall': 0.697100666306501, 'roc_auc': 0.8559421266170237, 'f1': 0.7002532561505066, 'TP': 3871, 'TN': 8662, 'FP': 1632, 'FN': 1682}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = CatBoostTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='Cat Boost + n1/5'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "    \n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d43a58-7c4c-47e2-8280-adc613852e75",
   "metadata": {},
   "source": [
    "## Light Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e870b20-3ed6-4268-8c30-fd73e0aa100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 43346, number of negative: 66914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2284\n",
      "[LightGBM] [Info] Number of data points in the train set: 110260, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.393125 -> initscore=-0.434194\n",
      "[LightGBM] [Info] Start training from score -0.434194\n",
      "LightGBM model trained.\n",
      "Model saved to ../models/trained-models/LightGBMTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.6995355484101465, 'recall': 0.7052043940212498, 'roc_auc': 0.852842109196537, 'f1': 0.7023585328670076, 'TP': 3916, 'TN': 8612, 'FP': 1682, 'FN': 1637}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = LightGBMTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='Light GBM Feature Selection .5'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "    \n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca194a5-8898-49de-8297-6e8316a32af1",
   "metadata": {},
   "source": [
    "## Elastic Net Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808a54d1-2542-4f0a-ac6a-e8b313ddee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net model trained.\n",
      "Model saved to ../models/trained-models/ElasticNetTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.0, 'recall': 0.0, 'roc_auc': 0.5, 'f1': 0.0, 'TP': 0, 'TN': 10294, 'FP': 0, 'FN': 5553}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtren\\anaconda3\\envs\\bayes-optimisation\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = ElasticNetTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='Elastic Net Trainer'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "    \n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac97020-c64c-4f0b-a985-d6bffb9391bf",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8ba952-0c84-41ff-9b7d-cfaed94f6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian model trained.\n",
      "Model saved to ../models/trained-models/BayesianModelTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.7606232294617564, 'recall': 0.2901134521880065, 'roc_auc': 0.790997903138805, 'f1': 0.4200234649980446, 'TP': 1611, 'TN': 9787, 'FP': 507, 'FN': 3942}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = BayesianModelTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='Gaussian Naive Bayes'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "    \n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5801634-2810-46c4-8d60-725aa9f71042",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9b45c6-224a-49b1-8a60-7d2c76d5c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model trained.\n",
      "Model saved to ../models/trained-models/KNNTrainer_model.joblib\n",
      "Training and evaluation completed.\n",
      "Metrics: {'precision': 0.6747879444143656, 'recall': 0.6733297316765712, 'roc_auc': 0.8113719653181516, 'f1': 0.6740580493960698, 'TP': 3739, 'TN': 8492, 'FP': 1802, 'FN': 1814}\n"
     ]
    }
   ],
   "source": [
    "# Initialize ModelTrainer with the number of features\n",
    "trainer = KNNTrainer()\n",
    "\n",
    "# Start MLflow run for tracking and logging\n",
    "with mlflow.start_run(run_name='KNN + n1/5'):\n",
    "    # Train the model\n",
    "    trainer.train(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the test set\n",
    "    probabilities = trainer.predict(X_test)\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate the model and log metrics\n",
    "    metrics = evaluate_model(y_test, probabilities)\n",
    "    log_metrics(metrics)\n",
    "    \n",
    "    print(\"Training and evaluation completed.\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e944757-38d6-42bb-aa04-e735f4b4a88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bayes-optimisation)",
   "language": "python",
   "name": "bayes-optimisation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
